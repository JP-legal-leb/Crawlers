{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bf44EJs9CNUg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "#Dependencies\n",
        "%pip install -q selenium webdriver-manager beautifulsoup4 pdf2docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJWShxHVCeKV",
        "outputId": "874f5d98-8f17-4ea8-ced1-ccf57bdc7815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåê Navigating to Nezams website...\n",
            "‚ùå Error: HTTPConnectionPool(host='localhost', port=64377): Read timed out. (read timeout=120)\n",
            "‚ùå Error: HTTPConnectionPool(host='localhost', port=64377): Read timed out. (read timeout=120)\n"
          ]
        }
      ],
      "source": [
        "# This section retrieves a fresh session cookie, fetches the entries' IDs, titles, and URLs, and saves them in a file \"Nezams_IDs.{date}.json\".\n",
        "\n",
        "import json\n",
        "import time\n",
        "import urllib.parse\n",
        "from datetime import datetime\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "OUTPUT_FILENAME_TEMPLATE = \"Nezams_IDs.{date}.json\"\n",
        "\n",
        "def fetch_and_save_ids():\n",
        "    # Set up Chrome options\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "    \n",
        "    # Initialize webdriver\n",
        "    service = Service(ChromeDriverManager().install())\n",
        "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "    \n",
        "    try:\n",
        "        print(\"üåê Navigating to Nezams website...\")\n",
        "        driver.get(\"https://nezams.com/\")\n",
        "        \n",
        "        # Wait for page to load\n",
        "        wait = WebDriverWait(driver, 10)\n",
        "        time.sleep(3)  # Give time for AJAX requests\n",
        "        \n",
        "        # Look for admin-ajax.php requests in network logs or try to find the data source\n",
        "        # Since we can't directly intercept network requests with Selenium like Playwright,\n",
        "        # let's try to find the data in the page source or make the AJAX request ourselves\n",
        "        \n",
        "        # First, let's try to find any JSON data embedded in the page\n",
        "        page_source = driver.page_source\n",
        "        \n",
        "        # Try to find script tags that might contain the data\n",
        "        from bs4 import BeautifulSoup\n",
        "        soup = BeautifulSoup(page_source, 'html.parser')\n",
        "        \n",
        "        # Look for common elements that might contain the systems data\n",
        "        systems = []\n",
        "        \n",
        "        # Try to find links or elements that represent the systems\n",
        "        # Common selectors for navigation menus or lists\n",
        "        selectors_to_try = [\n",
        "            'a[href*=\"nezam\"]',\n",
        "            'a[href*=\"system\"]', \n",
        "            '.menu-item a',\n",
        "            '.nav-item a',\n",
        "            'ul li a',\n",
        "            '[data-id]',\n",
        "            '.system-link'\n",
        "        ]\n",
        "        \n",
        "        for selector in selectors_to_try:\n",
        "            try:\n",
        "                elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                if elements:\n",
        "                    print(f\"‚úÖ Found {len(elements)} elements with selector: {selector}\")\n",
        "                    for i, elem in enumerate(elements[:10]):  # Limit to first 10 for testing\n",
        "                        try:\n",
        "                            text = elem.text.strip()\n",
        "                            href = elem.get_attribute('href')\n",
        "                            if text and href and 'nezam' in href.lower():\n",
        "                                systems.append({\n",
        "                                    \"id\": i + 1,\n",
        "                                    \"name\": text,\n",
        "                                    \"url\": href\n",
        "                                })\n",
        "                        except:\n",
        "                            continue\n",
        "                    if systems:\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        \n",
        "        # If we didn't find systems through selectors, try a different approach\n",
        "        if not systems:\n",
        "            print(\"‚ö† No systems found through standard selectors, trying alternative approach...\")\n",
        "            \n",
        "            # Try to execute JavaScript to get data\n",
        "            try:\n",
        "                # Look for any global variables or data structures\n",
        "                js_result = driver.execute_script(\"\"\"\n",
        "                    // Look for common global variables that might contain data\n",
        "                    var data = [];\n",
        "                    if (typeof window.nezams_data !== 'undefined') {\n",
        "                        return window.nezams_data;\n",
        "                    }\n",
        "                    if (typeof window.systems !== 'undefined') {\n",
        "                        return window.systems;\n",
        "                    }\n",
        "                    // Look for menu data\n",
        "                    var links = document.querySelectorAll('a[href*=\"nezam\"], a[href*=\"system\"]');\n",
        "                    for (var i = 0; i < links.length; i++) {\n",
        "                        if (links[i].href && links[i].textContent.trim()) {\n",
        "                            data.push({\n",
        "                                id: i + 1,\n",
        "                                name: links[i].textContent.trim(),\n",
        "                                url: links[i].href\n",
        "                            });\n",
        "                        }\n",
        "                    }\n",
        "                    return data;\n",
        "                \"\"\")\n",
        "                \n",
        "                if js_result and isinstance(js_result, list):\n",
        "                    systems = js_result[:20]  # Limit to first 20\n",
        "                    print(f\"‚úÖ Found {len(systems)} systems through JavaScript\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå JavaScript execution failed: {e}\")\n",
        "        \n",
        "        # Save the data\n",
        "        if systems:\n",
        "            today = datetime.now().strftime(\"%m.%d.%Y\")\n",
        "            filename = OUTPUT_FILENAME_TEMPLATE.format(date=today)\n",
        "            \n",
        "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(systems, f, ensure_ascii=False, indent=2)\n",
        "            \n",
        "            print(f\"‚úÖ Saved {len(systems)} entries to {filename}\")\n",
        "            \n",
        "            # Print first few entries for verification\n",
        "            print(\"\\nüìã First 5 entries:\")\n",
        "            for i, system in enumerate(systems[:5], 1):\n",
        "                print(f\"  {i}. {system['name']}\")\n",
        "                print(f\"     URL: {system['url']}\")\n",
        "        else:\n",
        "            print(\"‚ùå No systems found. The website structure might have changed.\")\n",
        "            print(\"üîç You may need to inspect the website manually to find the correct selectors.\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "# Run the function\n",
        "fetch_and_save_ids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "kmSL7PByD9gs",
        "outputId": "585f75f9-5181-438c-a4e9-2e7a66a2ce7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Nezams document scraping...\n",
            "üßπ Cleaned up existing Chrome processes\n",
            "Loaded 9 URLs from the JSON file: Nezams_IDs.09.19.2025.json\n",
            "\n",
            "üßπ Cleaned up existing Chrome processes\n",
            "Loaded 9 URLs from the JSON file: Nezams_IDs.09.19.2025.json\n",
            "\n",
            "üåê Processing (1/9): ID 2\n",
            "üåê Processing (1/9): ID 2\n",
            "‚úÖ Saved (1/9): ŸÜÿ∏ÿßŸÖ ÿßŸÑŸÜŸÇŸÑ ÿßŸÑÿ®ÿ±Ÿä ÿπŸÑŸâ ÿßŸÑÿ∑ÿ±ŸÇ (21953 chars)\n",
            "üåê Processing (2/9): ID 3\n",
            "‚úÖ Saved (1/9): ŸÜÿ∏ÿßŸÖ ÿßŸÑŸÜŸÇŸÑ ÿßŸÑÿ®ÿ±Ÿä ÿπŸÑŸâ ÿßŸÑÿ∑ÿ±ŸÇ (21953 chars)\n",
            "üåê Processing (2/9): ID 3\n",
            "‚úÖ Saved (2/9): ŸÜÿ∏ÿßŸÖ ÿßŸÑŸÖŸàÿßÿØ ÿßŸÑÿ®ÿ™ÿ±ŸàŸÑŸäÿ© ŸàÿßŸÑÿ®ÿ™ÿ±ŸàŸÉŸäŸÖÿßŸàŸäÿ© (17974 chars)\n",
            "üåê Processing (3/9): ID 4\n",
            "‚úÖ Saved (2/9): ŸÜÿ∏ÿßŸÖ ÿßŸÑŸÖŸàÿßÿØ ÿßŸÑÿ®ÿ™ÿ±ŸàŸÑŸäÿ© ŸàÿßŸÑÿ®ÿ™ÿ±ŸàŸÉŸäŸÖÿßŸàŸäÿ© (17974 chars)\n",
            "üåê Processing (3/9): ID 4\n",
            "‚úÖ Saved (3/9): ŸÜÿ∏ÿßŸÖ ÿßŸÑŸÇŸäÿßÿ≥ ŸàÿßŸÑŸÖÿπÿßŸäÿ±ÿ© (14686 chars)\n",
            "üåê Processing (4/9): ID 5\n",
            "‚úÖ Saved (3/9): ŸÜÿ∏ÿßŸÖ ÿßŸÑŸÇŸäÿßÿ≥ ŸàÿßŸÑŸÖÿπÿßŸäÿ±ÿ© (14686 chars)\n",
            "üåê Processing (4/9): ID 5\n",
            "‚úÖ Saved (4/9): ŸÜÿ∏ÿßŸÖ ÿßŸÑÿßÿ≥ÿ™ÿ´ŸÖÿßÿ± (12644 chars)\n",
            "üåê Processing (5/9): ID 6\n",
            "‚úÖ Saved (4/9): ŸÜÿ∏ÿßŸÖ ÿßŸÑÿßÿ≥ÿ™ÿ´ŸÖÿßÿ± (12644 chars)\n",
            "üåê Processing (5/9): ID 6\n",
            "‚úÖ Saved (5/9): ÿßŸÑŸÜÿ∏ÿßŸÖ ÿßŸÑÿ£ÿ≥ÿßÿ≥ ŸÑŸÖÿ≥ÿ™ÿ¥ŸÅŸâ ÿßŸÑŸÖŸÑŸÉ ÿÆÿßŸÑÿØ ÿßŸÑÿ™ÿÆÿµÿµŸä ŸÑŸÑÿπŸäŸàŸÜ ŸàŸÖÿ±ŸÉÿ≤ ÿßŸÑÿ£ÿ®ÿ≠ÿßÿ´ (ŸÖÿ§ÿ≥ÿ≥ÿ© ŸÖÿ≥ÿ™ŸÇŸÑÿ© ÿ∞ÿßÿ™ ÿ∑ÿ®Ÿäÿπÿ© ÿÆÿßÿµÿ© Ÿàÿ∫Ÿäÿ± ŸáÿßÿØŸÅ (18631 chars)\n",
            "üåê Processing (6/9): ID 7\n",
            "‚úÖ Saved (5/9): ÿßŸÑŸÜÿ∏ÿßŸÖ ÿßŸÑÿ£ÿ≥ÿßÿ≥ ŸÑŸÖÿ≥ÿ™ÿ¥ŸÅŸâ ÿßŸÑŸÖŸÑŸÉ ÿÆÿßŸÑÿØ ÿßŸÑÿ™ÿÆÿµÿµŸä ŸÑŸÑÿπŸäŸàŸÜ ŸàŸÖÿ±ŸÉÿ≤ ÿßŸÑÿ£ÿ®ÿ≠ÿßÿ´ (ŸÖÿ§ÿ≥ÿ≥ÿ© ŸÖÿ≥ÿ™ŸÇŸÑÿ© ÿ∞ÿßÿ™ ÿ∑ÿ®Ÿäÿπÿ© ÿÆÿßÿµÿ© Ÿàÿ∫Ÿäÿ± ŸáÿßÿØŸÅ (18631 chars)\n",
            "üåê Processing (6/9): ID 7\n",
            "‚úÖ Saved (6/9): ŸÜÿ∏ÿßŸÖ ÿ∂ÿ±Ÿäÿ®ÿ© ÿßŸÑÿ™ÿµÿ±ŸÅÿßÿ™ ÿßŸÑÿπŸÇÿßÿ±Ÿäÿ© (21949 chars)\n",
            "üåê Processing (7/9): ID 8\n",
            "‚úÖ Saved (6/9): ŸÜÿ∏ÿßŸÖ ÿ∂ÿ±Ÿäÿ®ÿ© ÿßŸÑÿ™ÿµÿ±ŸÅÿßÿ™ ÿßŸÑÿπŸÇÿßÿ±Ÿäÿ© (21949 chars)\n",
            "üåê Processing (7/9): ID 8\n",
            "‚ùå Failed (7/9) ID 8: HTTPConnectionPool(host='localhost', port=23889): Read timed out. (read timeout=120)\n",
            "üåê Processing (8/9): ID 9\n",
            "‚ùå Failed (7/9) ID 8: HTTPConnectionPool(host='localhost', port=23889): Read timed out. (read timeout=120)\n",
            "üåê Processing (8/9): ID 9\n",
            "‚ùå Failed (8/9) ID 9: HTTPConnectionPool(host='localhost', port=23889): Read timed out. (read timeout=120)\n",
            "üåê Processing (9/9): ID 10\n",
            "‚ùå Failed (8/9) ID 9: HTTPConnectionPool(host='localhost', port=23889): Read timed out. (read timeout=120)\n",
            "üåê Processing (9/9): ID 10\n",
            "‚úÖ Saved (9/9): ÿßŸÑŸÜÿ∏ÿßŸÖ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿä ŸÑŸÑÿ≠ŸÉŸÖ (14626 chars)\n",
            "\n",
            "üéâ Processing complete!\n",
            "üìä Successfully downloaded 7 out of 9 documents\n",
            "üìÅ Files saved in 'Nezams_Docs' folder\n",
            "‚úÖ Saved (9/9): ÿßŸÑŸÜÿ∏ÿßŸÖ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿä ŸÑŸÑÿ≠ŸÉŸÖ (14626 chars)\n",
            "\n",
            "üéâ Processing complete!\n",
            "üìä Successfully downloaded 7 out of 9 documents\n",
            "üìÅ Files saved in 'Nezams_Docs' folder\n",
            "üßπ Cleaned up existing Chrome processes\n",
            "üßπ Cleaned up existing Chrome processes\n"
          ]
        }
      ],
      "source": [
        "# This section loads the previously saved JSON file, visits each URL, and constructs a DOCX file with selected elements and proper formatting.\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from bs4 import BeautifulSoup\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "from docx.oxml.ns import qn\n",
        "\n",
        "# Output folder\n",
        "output_dir = \"Nezams_Docs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Selectors to remove\n",
        "unwanted_selectors = [\n",
        "    \"div.fontsize.no-print\",\n",
        "    \"span.share-icon\",\n",
        "    \"span.total-readers\",\n",
        "    \"div.subject-share\",\n",
        "    \"span.numbe-s\",\n",
        "    \"div#more-items\",\n",
        "    \"ul#subject-nav-links\"\n",
        "]\n",
        "\n",
        "def kill_chrome_processes():\n",
        "    \"\"\"Kill any existing Chrome processes\"\"\"\n",
        "    try:\n",
        "        subprocess.run([\"taskkill\", \"/f\", \"/im\", \"chrome.exe\"], \n",
        "                      capture_output=True, text=True)\n",
        "        subprocess.run([\"taskkill\", \"/f\", \"/im\", \"chromedriver.exe\"], \n",
        "                      capture_output=True, text=True)\n",
        "        time.sleep(2)\n",
        "        print(\"üßπ Cleaned up existing Chrome processes\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Save DOCX with RTL and right-aligned paragraphs\n",
        "def save_docx(title, body, filename):\n",
        "    doc = Document()\n",
        "    section = doc.sections[0]\n",
        "    section.right_to_left = True\n",
        "\n",
        "    style = doc.styles['Normal']\n",
        "    style.font.name = 'Arial'\n",
        "    style._element.rPr.rFonts.set(qn('w:eastAsia'), 'Arial')\n",
        "    style.font.size = Pt(14)\n",
        "\n",
        "    p_title = doc.add_paragraph()\n",
        "    p_title.paragraph_format.right_to_left = True\n",
        "    p_title.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
        "    p_title.add_run(title)\n",
        "\n",
        "    p_body = doc.add_paragraph()\n",
        "    p_body.paragraph_format.right_to_left = True\n",
        "    p_body.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
        "    p_body.add_run(body)\n",
        "\n",
        "    doc.save(filename)\n",
        "\n",
        "# Synchronous scraper using Selenium\n",
        "def scrape_and_save_all():\n",
        "    # Clean up any existing Chrome processes first\n",
        "    kill_chrome_processes()\n",
        "    \n",
        "    today = datetime.now().strftime(\"%m.%d.%Y\")\n",
        "    filename = f\"Nezams_IDs.{today}.json\"\n",
        "\n",
        "    try:\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        items_to_scrape = [item for item in data if 'url' in item and item['url']]\n",
        "        print(f\"Loaded {len(items_to_scrape)} URLs from the JSON file: {filename}\\n\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: JSON file '{filename}' not found. Please ensure the file exists.\")\n",
        "        return\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Error: JSON file is not valid.\")\n",
        "        return\n",
        "\n",
        "    # Set up Chrome options\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "    \n",
        "    # Initialize webdriver\n",
        "    service = Service(ChromeDriverManager().install())\n",
        "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "    \n",
        "    try:\n",
        "        successful_downloads = 0\n",
        "        \n",
        "        for count, item in enumerate(items_to_scrape, 1):\n",
        "            url = item['url']\n",
        "            item_id = item.get('id', 'N/A')\n",
        "            \n",
        "            try:\n",
        "                print(f\"üåê Processing ({count}/{len(items_to_scrape)}): ID {item_id}\")\n",
        "                \n",
        "                driver.get(url)\n",
        "                time.sleep(2)  # Wait for page to load\n",
        "                \n",
        "                html = driver.page_source\n",
        "                soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "                # Try to find title and content with various selectors\n",
        "                title_tag = soup.select_one(\"body > div.page > h1\")\n",
        "                if not title_tag:\n",
        "                    # Try alternative selectors for title\n",
        "                    title_tag = soup.select_one(\"h1\") or soup.select_one(\".page-title\") or soup.select_one(\"title\")\n",
        "                \n",
        "                content_div = soup.select_one(\"body > div.page > div.post-page > div\")\n",
        "                if not content_div:\n",
        "                    # Try alternative selectors for content\n",
        "                    content_div = soup.select_one(\".post-page\") or soup.select_one(\".content\") or soup.select_one(\"main\")\n",
        "                \n",
        "                if not title_tag or not content_div:\n",
        "                    print(f\"‚ö† No title or content found for ID {item_id}\")\n",
        "                    continue\n",
        "                    \n",
        "                title = title_tag.get_text(strip=True)\n",
        "\n",
        "                # Remove unwanted elements\n",
        "                for selector in unwanted_selectors:\n",
        "                    for tag in content_div.select(selector):\n",
        "                        tag.decompose()\n",
        "\n",
        "                # Handle nested spans\n",
        "                for outer in content_div.select('span.selectionShareable[style=\"color: #993300;\"]'):\n",
        "                    inner_spans = outer.select('span.selectionShareable')\n",
        "                    combined = ' '.join(s.get_text(strip=True) for s in inner_spans if s.get_text(strip=True))\n",
        "                    if combined:\n",
        "                        outer.string = combined\n",
        "                    for s in inner_spans:\n",
        "                        s.decompose()\n",
        "\n",
        "                body_text = content_div.get_text(separator=\"\\n\", strip=True)\n",
        "                \n",
        "                if not body_text.strip():\n",
        "                    print(f\"‚ö† No content text found for ID {item_id}\")\n",
        "                    continue\n",
        "\n",
        "                # Create safe filename\n",
        "                safe_title = title.replace(\"/\", \"-\").replace(\":\", \"ÿå\").replace(\"\\\\\", \"-\").replace(\"*\", \"\").replace(\"?\", \"\").replace(\"\\\"\", \"\").replace(\"<\", \"\").replace(\">\", \"\").replace(\"|\", \"\").strip()\n",
        "                safe_title = safe_title[:100]  # Limit filename length\n",
        "                \n",
        "                output_filename = os.path.join(output_dir, f\"{safe_title}.docx\")\n",
        "                save_docx(title, body_text, output_filename)\n",
        "                successful_downloads += 1\n",
        "                print(f\"‚úÖ Saved ({count}/{len(items_to_scrape)}): {safe_title} ({len(body_text)} chars)\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Failed ({count}/{len(items_to_scrape)}) ID {item_id}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\nüéâ Processing complete!\")\n",
        "        print(f\"üìä Successfully downloaded {successful_downloads} out of {len(items_to_scrape)} documents\")\n",
        "        print(f\"üìÅ Files saved in '{output_dir}' folder\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Fatal error: {str(e)}\")\n",
        "    finally:\n",
        "        driver.quit()\n",
        "        kill_chrome_processes()\n",
        "\n",
        "# Run the scraper\n",
        "print(\"üöÄ Starting Nezams document scraping...\")\n",
        "scrape_and_save_all()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
