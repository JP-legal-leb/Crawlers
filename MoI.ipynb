{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for proper Arabic text handling\n",
    "!pip install arabic-reshaper python-bidi ocrmypdf pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 114,
     "status": "aborted",
     "timestamp": 1751215644557,
     "user": {
      "displayName": "Mohammed Lababidi",
      "userId": "14108617793603566916"
     },
     "user_tz": -180
    },
    "id": "z5QF0t944DXn"
   },
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66692,
     "status": "ok",
     "timestamp": 1751215858573,
     "user": {
      "displayName": "Mohammed Lababidi",
      "userId": "14108617793603566916"
     },
     "user_tz": -180
    },
    "id": "mpYf025x49NB",
    "outputId": "3548d5e8-570e-4996-e67f-ce020cd2ab93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting download and conversion process...\n",
      "============================================================\n",
      "\n",
      "[1/6] Processing: https://misa.gov.sa/app/uploads/2025/07/Investor-Guide_12-04-ar.pdf\n",
      "üì• Downloaded: Investor-Guide_12-04-ar.pdf\n",
      "üîÑ Converting to Word with UTF-8 encoding...\n",
      "‚úÖ Converted: Investor-Guide_12-04-ar.docx\n",
      "\n",
      "[2/6] Processing: https://misa.gov.sa/app/uploads/2025/07/Investor-Guide_12-04-ar.pdf\n",
      "‚è≠Ô∏è  Already converted: Investor-Guide_12-04-ar.docx\n",
      "\n",
      "[3/6] Processing: https://misa.gov.sa/app/uploads/2025/05/investor-guide-12-03-ar.pdf\n",
      "üì• Downloaded: Investor-Guide_12-04-ar.pdf\n",
      "üîÑ Converting to Word with UTF-8 encoding...\n",
      "‚úÖ Converted: Investor-Guide_12-04-ar.docx\n",
      "\n",
      "[2/6] Processing: https://misa.gov.sa/app/uploads/2025/07/Investor-Guide_12-04-ar.pdf\n",
      "‚è≠Ô∏è  Already converted: Investor-Guide_12-04-ar.docx\n",
      "\n",
      "[3/6] Processing: https://misa.gov.sa/app/uploads/2025/05/investor-guide-12-03-ar.pdf\n",
      "üì• Downloaded: investor-guide-12-03-ar.pdf\n",
      "üîÑ Converting to Word with UTF-8 encoding...\n",
      "üì• Downloaded: investor-guide-12-03-ar.pdf\n",
      "üîÑ Converting to Word with UTF-8 encoding...\n",
      "‚úÖ Converted: investor-guide-12-03-ar.docx\n",
      "\n",
      "[4/6] Processing: https://misa.gov.sa/app/uploads/2024/12/ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿÆÿµŸàÿµŸäÿ©_ÿßŸÑÿßÿµÿØÿßÿ±-ÿßŸÑÿ´ÿßŸÑÿ´.pdf\n",
      "‚úÖ Converted: investor-guide-12-03-ar.docx\n",
      "\n",
      "[4/6] Processing: https://misa.gov.sa/app/uploads/2024/12/ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿÆÿµŸàÿµŸäÿ©_ÿßŸÑÿßÿµÿØÿßÿ±-ÿßŸÑÿ´ÿßŸÑÿ´.pdf\n",
      "üì• Downloaded: ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿÆÿµŸàÿµŸäÿ©_ÿßŸÑÿßÿµÿØÿßÿ±-ÿßŸÑÿ´ÿßŸÑÿ´.pdf\n",
      "üîÑ Converting to Word with UTF-8 encoding...\n",
      "‚úÖ Converted: ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿÆÿµŸàÿµŸäÿ©_ÿßŸÑÿßÿµÿØÿßÿ±-ÿßŸÑÿ´ÿßŸÑÿ´.docx\n",
      "\n",
      "[5/6] Processing: https://misa.gov.sa/app/uploads/2025/05/ÿßŸÑÿ¥ÿ±Ÿàÿ∑-ŸàÿßŸÑÿ£ÿ≠ŸÉÿßŸÖ-ÿßŸÑÿ∑ÿ®ÿπÿ©-03.pdf\n",
      "üì• Downloaded: ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿÆÿµŸàÿµŸäÿ©_ÿßŸÑÿßÿµÿØÿßÿ±-ÿßŸÑÿ´ÿßŸÑÿ´.pdf\n",
      "üîÑ Converting to Word with UTF-8 encoding...\n",
      "‚úÖ Converted: ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿÆÿµŸàÿµŸäÿ©_ÿßŸÑÿßÿµÿØÿßÿ±-ÿßŸÑÿ´ÿßŸÑÿ´.docx\n",
      "\n",
      "[5/6] Processing: https://misa.gov.sa/app/uploads/2025/05/ÿßŸÑÿ¥ÿ±Ÿàÿ∑-ŸàÿßŸÑÿ£ÿ≠ŸÉÿßŸÖ-ÿßŸÑÿ∑ÿ®ÿπÿ©-03.pdf\n",
      "üì• Downloaded: ÿßŸÑÿ¥ÿ±Ÿàÿ∑-ŸàÿßŸÑÿ£ÿ≠ŸÉÿßŸÖ-ÿßŸÑÿ∑ÿ®ÿπÿ©-03.pdf\n",
      "üîÑ Converting to Word with UTF-8 encoding...\n",
      "‚úÖ Converted: ÿßŸÑÿ¥ÿ±Ÿàÿ∑-ŸàÿßŸÑÿ£ÿ≠ŸÉÿßŸÖ-ÿßŸÑÿ∑ÿ®ÿπÿ©-03.docx\n",
      "\n",
      "[6/6] Processing: https://misa.gov.sa/app/uploads/2025/04/ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ-ÿßŸÑÿ¢ŸÖŸÜ.pdf\n",
      "üì• Downloaded: ÿßŸÑÿ¥ÿ±Ÿàÿ∑-ŸàÿßŸÑÿ£ÿ≠ŸÉÿßŸÖ-ÿßŸÑÿ∑ÿ®ÿπÿ©-03.pdf\n",
      "üîÑ Converting to Word with UTF-8 encoding...\n",
      "‚úÖ Converted: ÿßŸÑÿ¥ÿ±Ÿàÿ∑-ŸàÿßŸÑÿ£ÿ≠ŸÉÿßŸÖ-ÿßŸÑÿ∑ÿ®ÿπÿ©-03.docx\n",
      "\n",
      "[6/6] Processing: https://misa.gov.sa/app/uploads/2025/04/ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ-ÿßŸÑÿ¢ŸÖŸÜ.pdf\n",
      "üì• Downloaded: ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ-ÿßŸÑÿ¢ŸÖŸÜ.pdf\n",
      "üîÑ Converting to Word with UTF-8 encoding...\n",
      "‚úÖ Converted: ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ-ÿßŸÑÿ¢ŸÖŸÜ.docx\n",
      "\n",
      "============================================================\n",
      "üéâ All PDFs downloaded and converted successfully!\n",
      "üìÅ PDFs saved to: C:\\Users\\ali-d\\Desktop\\KSA\\MoI_pdfs\n",
      "üìÅ Word documents saved to: C:\\Users\\ali-d\\Desktop\\KSA\\MoI_docx\n",
      "üì• Downloaded: ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ-ÿßŸÑÿ¢ŸÖŸÜ.pdf\n",
      "üîÑ Converting to Word with UTF-8 encoding...\n",
      "‚úÖ Converted: ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ-ÿßŸÑÿ¢ŸÖŸÜ.docx\n",
      "\n",
      "============================================================\n",
      "üéâ All PDFs downloaded and converted successfully!\n",
      "üìÅ PDFs saved to: C:\\Users\\ali-d\\Desktop\\KSA\\MoI_pdfs\n",
      "üìÅ Word documents saved to: C:\\Users\\ali-d\\Desktop\\KSA\\MoI_docx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "from docx import Document\n",
    "from docx.oxml.ns import qn\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Your ScraperAPI Key\n",
    "SCRAPER_API_KEY = \"2ef897558ee673a53e6c66518eac104d\"  # Replace with your actual API key\n",
    "\n",
    "# Target website URL\n",
    "base_url = \"https://misa.gov.sa/ar/activities/laws-regulations/ar\"\n",
    "scraper_url = f\"http://api.scraperapi.com?api_key={SCRAPER_API_KEY}&url={base_url}\"\n",
    "\n",
    "# Folders to store the PDFs and Word documents (Windows path in current directory)\n",
    "download_folder = r\"C:\\Users\\ali-d\\Desktop\\KSA\\MoI_pdfs\"\n",
    "docx_folder = r\"C:\\Users\\ali-d\\Desktop\\KSA\\MoI_docx\"\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "os.makedirs(docx_folder, exist_ok=True)\n",
    "\n",
    "# Arabic text processing constants\n",
    "ARABIC_LETTERS = r\"[\\u0600-\\u06FF]\"\n",
    "\n",
    "def clean_arabic_text(text):\n",
    "    \"\"\"Clean and format Arabic text\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\u202A-\\u202E\\u200E\\u200F\\u2066-\\u2069]\", \"\", text)\n",
    "    text = re.sub(r'\\s*[\\(\\)]\\s*', '', text)\n",
    "    replacements = {\n",
    "        \"ÿ£ŸÑŸä\": \"ÿ£Ÿä\", \"ŸÑÿ£ŸÑŸä\": \"ŸÑŸÑÿ£\", \"ÿ°ÿß\": \"ÿßÿ°\", \"ÿ§\": \"ÿ°\", \"Ÿâÿ°\": \"ÿ¶Ÿä\",\n",
    "        \"ÿ©ÿßŸÑ\": \"ÿ© ÿßŸÑ\", \"ÿßÿßÿ°\": \"ÿßÿ°\", \"ÿßŸÑŸÖÿ°ÿ≥ÿ≥ÿßÿ™\": \"ÿßŸÑŸÖÿ§ÿ≥ÿ≥ÿßÿ™\",\n",
    "        \"ŸÖÿ≥ÿ°ŸàŸÑŸäÿ©\": \"ŸÖÿ≥ÿ§ŸàŸÑŸäÿ©\", \"ÿ™ÿ°ÿØŸä\": \"ÿ™ÿ§ÿØŸä\", \"ÿßŸà\": \"ÿ£Ÿà\",\n",
    "        \"ŸàÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿ™\": \"ŸàÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™\", \"ÿ•ÿ¨ÿ±ÿßÿ°ÿ™\": \"ÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™\"\n",
    "    }\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    text = re.sub(r'\\s*([.,:;!?ÿåÿõÿü])\\s*', r'\\1', text)\n",
    "    text = re.sub(r'([.,:;!?ÿåÿõÿü])([%s])' % ARABIC_LETTERS, r'\\1 \\2', text)\n",
    "    text = re.sub(r\"([%s])([A-Z])\" % ARABIC_LETTERS, r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([a-zA-Z])([%s])\" % ARABIC_LETTERS, r\"\\1 \\2\", text)\n",
    "    arabic_digits = str.maketrans(\"0123456789\", \"Ÿ†Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ©\")\n",
    "    text = text.translate(arabic_digits)\n",
    "    if re.fullmatch(r\"^\\s*\\d+\\s*$\", text) or re.fullmatch(r\"^\\s*[^\\u0600-\\u06FF]{1,5}\\s*$\", text):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def set_rtl_paragraph(paragraph):\n",
    "    \"\"\"Set paragraph to Right-To-Left alignment for Arabic\"\"\"\n",
    "    paragraph.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
    "    p = paragraph._element\n",
    "    p.set(qn(\"w:rtl\"), \"1\")\n",
    "\n",
    "def convert_pdf_to_docx(pdf_path, docx_path):\n",
    "    \"\"\"Convert PDF to Word document with Arabic formatting and proper UTF-8 encoding\"\"\"\n",
    "    try:\n",
    "        doc = Document()\n",
    "        \n",
    "        # Open PDF - PyMuPDF handles UTF-8 automatically\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        \n",
    "        for page_num in range(pdf_document.page_count):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            \n",
    "            # Extract text blocks with positioning info to preserve RTL order\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "            \n",
    "            # Sort blocks by vertical position (top to bottom)\n",
    "            blocks = sorted(blocks, key=lambda b: b[1])\n",
    "            \n",
    "            current_paragraph_lines = []\n",
    "            \n",
    "            for block in blocks:\n",
    "                # block format: (x0, y0, x1, y1, \"text\", block_no, block_type)\n",
    "                if len(block) >= 5:\n",
    "                    text = block[4].strip()\n",
    "                    \n",
    "                    if not text:\n",
    "                        if current_paragraph_lines:\n",
    "                            paragraph_text = \" \".join(current_paragraph_lines)\n",
    "                            cleaned_paragraph = clean_arabic_text(paragraph_text)\n",
    "                            if cleaned_paragraph:\n",
    "                                p = doc.add_paragraph(cleaned_paragraph)\n",
    "                                set_rtl_paragraph(p)\n",
    "                                # Set font that supports Arabic\n",
    "                                for run in p.runs:\n",
    "                                    run.font.name = 'Arial'\n",
    "                                    run.font.size = 304800  # 12pt in EMUs\n",
    "                            current_paragraph_lines = []\n",
    "                        continue\n",
    "                    \n",
    "                    # Split into lines\n",
    "                    lines = text.splitlines()\n",
    "                    \n",
    "                    for line in lines:\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "                        \n",
    "                        # Check if it's a new list item or heading\n",
    "                        is_new_list_item = False\n",
    "                        if re.match(r\"^\\s*([Ÿ†-Ÿ©0-9]+\\.|\\*|‚Ä¢)\\s*\", line):\n",
    "                            is_new_list_item = True\n",
    "                        elif re.match(r\"^\\s*([ÿ£-Ÿä] -|-)\\s*\", line):\n",
    "                            is_new_list_item = True\n",
    "                        \n",
    "                        if is_new_list_item and current_paragraph_lines:\n",
    "                            paragraph_text = \" \".join(current_paragraph_lines)\n",
    "                            cleaned_paragraph = clean_arabic_text(paragraph_text)\n",
    "                            if cleaned_paragraph:\n",
    "                                p = doc.add_paragraph(cleaned_paragraph)\n",
    "                                set_rtl_paragraph(p)\n",
    "                                for run in p.runs:\n",
    "                                    run.font.name = 'Arial'\n",
    "                                    run.font.size = 304800\n",
    "                            current_paragraph_lines = []\n",
    "                        \n",
    "                        current_paragraph_lines.append(line)\n",
    "            \n",
    "            # Add any remaining text from this page\n",
    "            if current_paragraph_lines:\n",
    "                paragraph_text = \" \".join(current_paragraph_lines)\n",
    "                cleaned_paragraph = clean_arabic_text(paragraph_text)\n",
    "                if cleaned_paragraph:\n",
    "                    p = doc.add_paragraph(cleaned_paragraph)\n",
    "                    set_rtl_paragraph(p)\n",
    "                    for run in p.runs:\n",
    "                        run.font.name = 'Arial'\n",
    "                        run.font.size = 304800\n",
    "        \n",
    "        pdf_document.close()\n",
    "        doc.save(docx_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Conversion error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Request the page via ScraperAPI\n",
    "response = requests.get(scraper_url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all PDF links\n",
    "pdf_links = []\n",
    "for link in soup.find_all(\"a\", href=True):\n",
    "    href = link[\"href\"]\n",
    "    if href.lower().endswith(\".pdf\"):  # Ensure it's a PDF link\n",
    "        pdf_links.append(urljoin(base_url, href))\n",
    "\n",
    "# Function to download PDFs and convert to Word immediately\n",
    "def download_pdf(url, retries=3, delay=5):\n",
    "    filename = os.path.join(download_folder, url.split(\"/\")[-1])\n",
    "    docx_filename = os.path.join(docx_folder, url.split(\"/\")[-1].replace(\".pdf\", \".docx\"))\n",
    "    \n",
    "    # Skip if Word document already exists\n",
    "    if os.path.exists(docx_filename):\n",
    "        print(f\"‚è≠Ô∏è  Already converted: {os.path.basename(docx_filename)}\")\n",
    "        return\n",
    "    \n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            # Request with proper encoding headers for Arabic content\n",
    "            response = requests.get(url, headers={'Accept-Charset': 'utf-8'})\n",
    "            if response.status_code == 200:\n",
    "                # Save PDF with binary mode (preserves encoding)\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"üì• Downloaded: {os.path.basename(filename)}\")\n",
    "                \n",
    "                # Convert to Word immediately with UTF-8 support\n",
    "                print(f\"üîÑ Converting to Word with UTF-8 encoding...\")\n",
    "                if convert_pdf_to_docx(filename, docx_filename):\n",
    "                    print(f\"‚úÖ Converted: {os.path.basename(docx_filename)}\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è  Conversion failed for {os.path.basename(filename)}\")\n",
    "                \n",
    "                return  # Successfully downloaded and converted\n",
    "            else:\n",
    "                print(f\"Attempt {i+1} failed for {url} with status code: {response.status_code}\")\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"Attempt {i+1} failed for {url} with connection error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {i+1} failed with error: {e}\")\n",
    "        time.sleep(delay) # Wait before retrying\n",
    "    print(f\"Failed to download {url} after {retries} attempts.\")\n",
    "\n",
    "\n",
    "# Download all PDFs and convert to Word\n",
    "print(\"üöÄ Starting download and conversion process...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, pdf_url in enumerate(pdf_links, 1):\n",
    "    print(f\"\\n[{i}/{len(pdf_links)}] Processing: {pdf_url}\")\n",
    "    download_pdf(pdf_url)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ All PDFs downloaded and converted successfully!\")\n",
    "print(f\"üìÅ PDFs saved to: {download_folder}\")\n",
    "print(f\"üìÅ Word documents saved to: {docx_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Advanced PDF Text Extraction\n",
      "================================================================================\n",
      "üìÅ Source folder: C:\\Users\\ali-d\\Desktop\\KSA\\MoI_pdfs\n",
      "üìÅ Output folder: C:\\Users\\ali-d\\Desktop\\KSA\\MoI_extracted_text\n",
      "\n",
      "‚öôÔ∏è Extraction Rules:\n",
      "   ‚Ä¢ include_images: False\n",
      "   ‚Ä¢ include_links: False\n",
      "   ‚Ä¢ preserve_formatting: True\n",
      "   ‚Ä¢ preserve_structure: True\n",
      "   ‚Ä¢ detect_language: True\n",
      "   ‚Ä¢ remove_headers_footers: True\n",
      "   ‚Ä¢ min_text_length: 3\n",
      "\n",
      "üìã Processing Steps:\n",
      "   1. Check if PDF is text-based or image-based (scanned)\n",
      "   2. Skip image-based PDFs (no extractable text)\n",
      "   3. Extract text from text-based PDFs\n",
      "   4. Apply OCR fallback if text quality is poor\n",
      "   5. Save as TXT and DOCX with proper Arabic formatting\n",
      "================================================================================\n",
      "\n",
      "üìö Found 5 PDF files\n",
      "\n",
      "\n",
      "[1/5] investor-guide-12-03-ar.pdf\n",
      "  üîç Checking PDF type...\n",
      "  ‚úÖ Text-based PDF: ~527 chars/page, 4 images/page\n",
      "\n",
      "üìñ Processing: investor-guide-12-03-ar.pdf\n",
      "  ‚úÖ Extracted 58 pages\n",
      "  üìä Arabic blocks: 1242\n",
      "  üìä English blocks: 0\n",
      "  üìä Mixed blocks: 8\n",
      "  üíæ Saved TXT: investor-guide-12-03-ar.txt\n",
      "  üíæ Saved DOCX: investor-guide-12-03-ar.docx\n",
      "\n",
      "[2/5] Investor-Guide_12-04-ar.pdf\n",
      "  üîç Checking PDF type...\n",
      "  ‚úÖ Text-based PDF: ~527 chars/page, 4 images/page\n",
      "\n",
      "üìñ Processing: Investor-Guide_12-04-ar.pdf\n",
      "  ‚úÖ Extracted 58 pages\n",
      "  üìä Arabic blocks: 1244\n",
      "  üìä English blocks: 0\n",
      "  üìä Mixed blocks: 8\n",
      "  üíæ Saved TXT: Investor-Guide_12-04-ar.txt\n",
      "  üíæ Saved DOCX: Investor-Guide_12-04-ar.docx\n",
      "\n",
      "[3/5] ÿßŸÑÿ¥ÿ±Ÿàÿ∑-ŸàÿßŸÑÿ£ÿ≠ŸÉÿßŸÖ-ÿßŸÑÿ∑ÿ®ÿπÿ©-03.pdf\n",
      "  üîç Checking PDF type...\n",
      "  ‚úÖ Text-based PDF: ~1689 chars/page, 1 images/page\n",
      "\n",
      "üìñ Processing: ÿßŸÑÿ¥ÿ±Ÿàÿ∑-ŸàÿßŸÑÿ£ÿ≠ŸÉÿßŸÖ-ÿßŸÑÿ∑ÿ®ÿπÿ©-03.pdf\n",
      "  ‚úÖ Extracted 8 pages\n",
      "  üìä Arabic blocks: 86\n",
      "  üìä English blocks: 0\n",
      "  üìä Mixed blocks: 0\n",
      "  üíæ Saved TXT: ÿßŸÑÿ¥ÿ±Ÿàÿ∑-ŸàÿßŸÑÿ£ÿ≠ŸÉÿßŸÖ-ÿßŸÑÿ∑ÿ®ÿπÿ©-03.txt\n",
      "  üíæ Saved DOCX: ÿßŸÑÿ¥ÿ±Ÿàÿ∑-ŸàÿßŸÑÿ£ÿ≠ŸÉÿßŸÖ-ÿßŸÑÿ∑ÿ®ÿπÿ©-03.docx\n",
      "\n",
      "[4/5] ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ-ÿßŸÑÿ¢ŸÖŸÜ.pdf\n",
      "  üîç Checking PDF type...\n",
      "  ‚úÖ Text-based PDF: ~1942 chars/page, 4 images/page\n",
      "\n",
      "üìñ Processing: ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ-ÿßŸÑÿ¢ŸÖŸÜ.pdf\n",
      "  ‚úÖ Extracted 5 pages\n",
      "  üìä Arabic blocks: 65\n",
      "  üìä English blocks: 0\n",
      "  üìä Mixed blocks: 2\n",
      "  üíæ Saved TXT: ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ-ÿßŸÑÿ¢ŸÖŸÜ.txt\n",
      "  üíæ Saved DOCX: ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ-ÿßŸÑÿ¢ŸÖŸÜ.docx\n",
      "\n",
      "[5/5] ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿÆÿµŸàÿµŸäÿ©_ÿßŸÑÿßÿµÿØÿßÿ±-ÿßŸÑÿ´ÿßŸÑÿ´.pdf\n",
      "  üîç Checking PDF type...\n",
      "  ‚úÖ Text-based PDF: ~1043 chars/page, 2 images/page\n",
      "\n",
      "üìñ Processing: ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿÆÿµŸàÿµŸäÿ©_ÿßŸÑÿßÿµÿØÿßÿ±-ÿßŸÑÿ´ÿßŸÑÿ´.pdf\n",
      "  ‚úÖ Extracted 12 pages\n",
      "  üìä Arabic blocks: 238\n",
      "  üìä English blocks: 0\n",
      "  üìä Mixed blocks: 10\n",
      "  üíæ Saved TXT: ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿÆÿµŸàÿµŸäÿ©_ÿßŸÑÿßÿµÿØÿßÿ±-ÿßŸÑÿ´ÿßŸÑÿ´.txt\n",
      "  üíæ Saved DOCX: ÿ≥Ÿäÿßÿ≥ÿ©-ÿßŸÑÿÆÿµŸàÿµŸäÿ©_ÿßŸÑÿßÿµÿØÿßÿ±-ÿßŸÑÿ´ÿßŸÑÿ´.docx\n",
      "\n",
      "================================================================================\n",
      "üéâ Extraction Complete!\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Total PDFs found: 5\n",
      "   ‚Ä¢ Successfully processed: 5\n",
      "   ‚Ä¢ Skipped (image-based): 0\n",
      "   ‚Ä¢ Failed: 0\n",
      "üìÅ All extracted files saved to: C:\\Users\\ali-d\\Desktop\\KSA\\MoI_extracted_text\n"
     ]
    }
   ],
   "source": [
    "# Advanced PDF Text Extraction with Format Preservation\n",
    "# Supports Arabic & English, excludes images, preserves structure\n",
    "# Enhanced with Arabic shaping, BiDi, and OCR fallback\n",
    "\n",
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from docx import Document\n",
    "from docx.shared import Pt, RGBColor\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.oxml.ns import qn\n",
    "from docx.oxml import OxmlElement\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Arabic text handling\n",
    "from bidi.algorithm import get_display\n",
    "import arabic_reshaper\n",
    "\n",
    "# Initialize Arabic reshaper for proper letter joining\n",
    "RESHAPER = arabic_reshaper.ArabicReshaper()\n",
    "\n",
    "# Configuration\n",
    "pdf_folder = r\"C:\\Users\\ali-d\\Desktop\\KSA\\MoI_pdfs\"\n",
    "output_folder = r\"C:\\Users\\ali-d\\Desktop\\KSA\\MoI_extracted_text\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Extraction rules\n",
    "EXTRACTION_RULES = {\n",
    "    'include_images': False,  # Do not include image URLs/references\n",
    "    'include_links': False,   # Do not include hyperlinks\n",
    "    'preserve_formatting': True,  # Preserve bold, italic, font sizes\n",
    "    'preserve_structure': True,   # Preserve paragraphs, lists, spacing\n",
    "    'detect_language': True,      # Detect Arabic vs English\n",
    "    'remove_headers_footers': True,  # Remove page headers/footers\n",
    "    'min_text_length': 3,  # Minimum characters to consider as text\n",
    "}\n",
    "\n",
    "def is_arabic(text):\n",
    "    \"\"\"Check if text contains Arabic characters\"\"\"\n",
    "    arabic_pattern = re.compile(r'[\\u0600-\\u06FF]')\n",
    "    return bool(arabic_pattern.search(text))\n",
    "\n",
    "def fix_arabic_shaping(text: str) -> str:\n",
    "    \"\"\"Shape Arabic letters and apply BiDi so words display correctly.\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    # Reshape (joins letters into contextual forms) then reorder for display\n",
    "    try:\n",
    "        reshaped = RESHAPER.reshape(text)\n",
    "        return get_display(reshaped)\n",
    "    except Exception as e:\n",
    "        # Fallback to original text if shaping fails\n",
    "        return text\n",
    "\n",
    "def is_header_footer(block, page_height, threshold=50):\n",
    "    \"\"\"Detect if text block is likely a header or footer\"\"\"\n",
    "    y0 = block[1]  # Top position\n",
    "    y1 = block[3]  # Bottom position\n",
    "    \n",
    "    # Check if in top or bottom margin\n",
    "    if y0 < threshold or y1 > (page_height - threshold):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    \"\"\"Clean extracted text with rules\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove control characters\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    \n",
    "    # Remove SOME invisible Unicode characters but keep RTL marks (U+200F, U+202B)\n",
    "    # that help with rendering\n",
    "    text = re.sub(r'[\\u200b-\\u200e\\u2010-\\u202a\\u202c-\\u202e\\u2060-\\u206f\\ufeff]', '', text)\n",
    "    \n",
    "    # Fix common Arabic OCR issues\n",
    "    replacements = {\n",
    "        \"ÿ£ŸÑŸä\": \"ÿ£Ÿä\", \"ŸÑÿ£ŸÑŸä\": \"ŸÑŸÑÿ£\", \"ÿ°ÿß\": \"ÿßÿ°\", \"Ÿâÿ°\": \"ÿ¶Ÿä\",\n",
    "        \"ÿ©ÿßŸÑ\": \"ÿ© ÿßŸÑ\", \"ÿßÿßÿ°\": \"ÿßÿ°\", \"ÿßŸÑŸÖÿ°ÿ≥ÿ≥ÿßÿ™\": \"ÿßŸÑŸÖÿ§ÿ≥ÿ≥ÿßÿ™\",\n",
    "        \"ŸÖÿ≥ÿ°ŸàŸÑŸäÿ©\": \"ŸÖÿ≥ÿ§ŸàŸÑŸäÿ©\", \"ÿ™ÿ°ÿØŸä\": \"ÿ™ÿ§ÿØŸä\", \"ÿßŸà\": \"ÿ£Ÿà\",\n",
    "        \"ŸàÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿ™\": \"ŸàÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™\", \"ÿ•ÿ¨ÿ±ÿßÿ°ÿ™\": \"ÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™\"\n",
    "    }\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    \n",
    "    # Normalize whitespace but preserve single spaces\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Max 2 newlines\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def is_pdf_text_based(pdf_path, sample_pages=3):\n",
    "    \"\"\"\n",
    "    Check if PDF is text-based or image-based (scanned).\n",
    "    Returns: (is_text_based: bool, text_ratio: float, details: str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_pages = len(doc)\n",
    "        pages_to_check = min(sample_pages, total_pages)\n",
    "        \n",
    "        text_chars = 0\n",
    "        image_count = 0\n",
    "        \n",
    "        for page_num in range(pages_to_check):\n",
    "            page = doc.load_page(page_num)\n",
    "            \n",
    "            # Count text characters\n",
    "            text = page.get_text(\"text\")\n",
    "            text_chars += len(text.strip())\n",
    "            \n",
    "            # Count images\n",
    "            image_list = page.get_images()\n",
    "            image_count += len(image_list)\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        # Calculate average per page\n",
    "        avg_text_per_page = text_chars / pages_to_check\n",
    "        avg_images_per_page = image_count / pages_to_check\n",
    "        \n",
    "        # Determine if text-based\n",
    "        # Text-based: has significant text (>100 chars/page) OR has text and few images\n",
    "        is_text_based = avg_text_per_page > 100\n",
    "        \n",
    "        if is_text_based:\n",
    "            details = f\"‚úÖ Text-based PDF: ~{int(avg_text_per_page)} chars/page, {int(avg_images_per_page)} images/page\"\n",
    "        else:\n",
    "            details = f\"‚ùå Image-based PDF (scanned): ~{int(avg_text_per_page)} chars/page, {int(avg_images_per_page)} images/page\"\n",
    "        \n",
    "        return is_text_based, avg_text_per_page, details\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, 0, f\"‚ùå Error checking PDF type: {e}\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, rules=EXTRACTION_RULES):\n",
    "    \"\"\"\n",
    "    Extract text from PDF with advanced formatting and rules\n",
    "    Returns structured data with text blocks and metadata\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìñ Processing: {os.path.basename(pdf_path)}\")\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        extracted_data = {\n",
    "            'filename': os.path.basename(pdf_path),\n",
    "            'pages': [],\n",
    "            'total_pages': len(doc),\n",
    "            'languages': set(),\n",
    "            'statistics': {'arabic_blocks': 0, 'english_blocks': 0, 'mixed_blocks': 0}\n",
    "        }\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            page_data = {\n",
    "                'page_number': page_num + 1,\n",
    "                'blocks': []\n",
    "            }\n",
    "            \n",
    "            # Get text blocks with position and formatting info\n",
    "            # sort=True ‚Üí better logical reading order\n",
    "            blocks = page.get_text(\"dict\", flags=fitz.TEXT_PRESERVE_WHITESPACE, sort=True)\n",
    "            \n",
    "            # Get page dimensions for header/footer detection\n",
    "            page_height = page.rect.height\n",
    "            \n",
    "            for block in blocks.get(\"blocks\", []):\n",
    "                # Skip images if rule is set\n",
    "                if block.get(\"type\") == 1 and not rules['include_images']:\n",
    "                    continue\n",
    "                \n",
    "                # Process text blocks\n",
    "                if block.get(\"type\") == 0:  # Text block\n",
    "                    block_bbox = block.get(\"bbox\", [0, 0, 0, 0])\n",
    "                    \n",
    "                    # Skip headers/footers if rule is set\n",
    "                    if rules['remove_headers_footers'] and is_header_footer(block_bbox, page_height):\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract text from lines - keep in LOGICAL order for storage\n",
    "                    block_text = \"\"\n",
    "                    for line in block.get(\"lines\", []):\n",
    "                        line_parts = []\n",
    "                        for span in line.get(\"spans\", []):\n",
    "                            text = span.get(\"text\", \"\")\n",
    "                            \n",
    "                            # Apply minimum length rule\n",
    "                            if len(text.strip()) < rules['min_text_length']:\n",
    "                                continue\n",
    "                            \n",
    "                            # Clean text first\n",
    "                            text = clean_extracted_text(text)\n",
    "                            \n",
    "                            # DO NOT shape here - keep logical order\n",
    "                            # We'll apply shaping when saving to TXT/DOCX\n",
    "                            \n",
    "                            if text:\n",
    "                                line_parts.append(text)\n",
    "                        \n",
    "                        # Join the span texts back for that visual line\n",
    "                        line_text = \" \".join(p for p in line_parts if p.strip())\n",
    "                        if line_text:\n",
    "                            block_text += line_text + \"\\n\"\n",
    "                    \n",
    "                    block_text = block_text.strip()\n",
    "                    \n",
    "                    if block_text:\n",
    "                        # Detect language\n",
    "                        has_arabic = is_arabic(block_text)\n",
    "                        has_english = bool(re.search(r'[a-zA-Z]', block_text))\n",
    "                        \n",
    "                        if has_arabic and has_english:\n",
    "                            language = \"mixed\"\n",
    "                            extracted_data['statistics']['mixed_blocks'] += 1\n",
    "                        elif has_arabic:\n",
    "                            language = \"arabic\"\n",
    "                            extracted_data['statistics']['arabic_blocks'] += 1\n",
    "                        elif has_english:\n",
    "                            language = \"english\"\n",
    "                            extracted_data['statistics']['english_blocks'] += 1\n",
    "                        else:\n",
    "                            language = \"unknown\"\n",
    "                        \n",
    "                        extracted_data['languages'].add(language)\n",
    "                        \n",
    "                        page_data['blocks'].append({\n",
    "                            'text': block_text,\n",
    "                            'language': language,\n",
    "                            'position': block_bbox\n",
    "                        })\n",
    "            \n",
    "            if page_data['blocks']:\n",
    "                extracted_data['pages'].append(page_data)\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        print(f\"  ‚úÖ Extracted {len(extracted_data['pages'])} pages\")\n",
    "        print(f\"  üìä Arabic blocks: {extracted_data['statistics']['arabic_blocks']}\")\n",
    "        print(f\"  üìä English blocks: {extracted_data['statistics']['english_blocks']}\")\n",
    "        print(f\"  üìä Mixed blocks: {extracted_data['statistics']['mixed_blocks']}\")\n",
    "        \n",
    "        return extracted_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_to_txt(extracted_data, output_path):\n",
    "    \"\"\"Save extracted text to plain text file with visual Arabic shaping\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Document: {extracted_data['filename']}\\n\")\n",
    "        f.write(f\"Total Pages: {extracted_data['total_pages']}\\n\")\n",
    "        f.write(f\"Languages: {', '.join(extracted_data['languages'])}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        for page_data in extracted_data['pages']:\n",
    "            f.write(f\"\\n--- Page {page_data['page_number']} ---\\n\\n\")\n",
    "            \n",
    "            for block in page_data['blocks']:\n",
    "                # Apply visual reshaping for TXT output\n",
    "                text = block['text']\n",
    "                if block['language'] in ['arabic', 'mixed'] and is_arabic(text):\n",
    "                    text = fix_arabic_shaping(text)\n",
    "                f.write(text + \"\\n\\n\")\n",
    "\n",
    "def set_run_rtl(run):\n",
    "    \"\"\"Set run-level RTL for proper Arabic rendering\"\"\"\n",
    "    rPr = run._element.get_or_add_rPr()\n",
    "    rtl = OxmlElement('w:rtl')\n",
    "    rtl.set(qn('w:val'), '1')\n",
    "    rPr.append(rtl)\n",
    "\n",
    "def set_run_rtl_and_font(run, font_name=\"Arial\"):\n",
    "    \"\"\"Set run-level RTL and complex-script fonts\"\"\"\n",
    "    rPr = run._element.get_or_add_rPr()\n",
    "    \n",
    "    # Set RTL\n",
    "    rtl = OxmlElement('w:rtl')\n",
    "    rtl.set(qn('w:val'), '1')\n",
    "    rPr.append(rtl)\n",
    "    \n",
    "    # Set complex-script fonts\n",
    "    rFonts = rPr.find(qn('w:rFonts'))\n",
    "    if rFonts is None:\n",
    "        rFonts = OxmlElement('w:rFonts')\n",
    "        rPr.append(rFonts)\n",
    "    \n",
    "    rFonts.set(qn('w:cs'), font_name)\n",
    "    rFonts.set(qn('w:ascii'), font_name)\n",
    "    rFonts.set(qn('w:hAnsi'), font_name)\n",
    "    run.font.name = font_name\n",
    "\n",
    "def set_paragraph_rtl(p):\n",
    "    \"\"\"Set paragraph-level RTL\"\"\"\n",
    "    p.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
    "    pPr = p._element.get_or_add_pPr()\n",
    "    bidi = OxmlElement('w:bidi')\n",
    "    bidi.set(qn('w:val'), '1')\n",
    "    pPr.append(bidi)\n",
    "\n",
    "def to_arabic_indic_digits(text):\n",
    "    \"\"\"Convert Western digits (0-9) to Arabic-Indic (Ÿ†-Ÿ©)\"\"\"\n",
    "    trans = str.maketrans(\"0123456789\", \"Ÿ†Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ©\")\n",
    "    return text.translate(trans)\n",
    "\n",
    "def save_to_docx(extracted_data, output_path):\n",
    "    \"\"\"\n",
    "    Save extracted text to Word with proper RTL formatting.\n",
    "    Uses same visual text as TXT (with arabic reshaping) + RTL formatting + Arabic digits.\n",
    "    \"\"\"\n",
    "    doc = Document()\n",
    "    \n",
    "    # Add title\n",
    "    title = doc.add_paragraph(extracted_data['filename'])\n",
    "    title.style = 'Title'\n",
    "    \n",
    "    # Add metadata\n",
    "    meta = doc.add_paragraph(f\"Total Pages: {extracted_data['total_pages']} | Languages: {', '.join(extracted_data['languages'])}\")\n",
    "    meta.style = 'Subtitle'\n",
    "    \n",
    "    for page_data in extracted_data['pages']:\n",
    "        # Page header\n",
    "        page_header = doc.add_paragraph(f\"Page {page_data['page_number']}\")\n",
    "        page_header.style = 'Heading 1'\n",
    "        \n",
    "        for block in page_data['blocks']:\n",
    "            txt = block['text']\n",
    "            \n",
    "            if block['language'] in ['arabic', 'mixed']:\n",
    "                # Apply visual reshaping (same as TXT)\n",
    "                if is_arabic(txt):\n",
    "                    txt = fix_arabic_shaping(txt)\n",
    "                \n",
    "                # Convert numbers to Arabic-Indic for proper alignment\n",
    "                txt = to_arabic_indic_digits(txt)\n",
    "                \n",
    "                # Add paragraph with RTL formatting\n",
    "                p = doc.add_paragraph(txt)\n",
    "                set_paragraph_rtl(p)\n",
    "                \n",
    "                for run in p.runs:\n",
    "                    set_run_rtl_and_font(run, 'Arial')\n",
    "                    run.font.size = Pt(12)\n",
    "            else:\n",
    "                # English text: LTR formatting\n",
    "                p = doc.add_paragraph(txt)\n",
    "                p.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "                for run in p.runs:\n",
    "                    run.font.name = 'Calibri'\n",
    "                    run.font.size = Pt(11)\n",
    "    \n",
    "    doc.save(output_path)\n",
    "\n",
    "def needs_ocr(extracted_data):\n",
    "    \"\"\"Check if PDF needs OCR fallback based on text quality\"\"\"\n",
    "    arabic_chars = sum(\n",
    "        ch >= '\\u0600' and ch <= '\\u06FF'\n",
    "        for page in extracted_data['pages']\n",
    "        for b in page['blocks'] \n",
    "        for ch in b['text']\n",
    "    )\n",
    "    total_chars = sum(\n",
    "        len(b['text']) \n",
    "        for page in extracted_data['pages'] \n",
    "        for b in page['blocks']\n",
    "    )\n",
    "    \n",
    "    # Heuristic: lots of Arabic but very little usable text ‚Üí try OCR\n",
    "    return arabic_chars > 200 and total_chars < 500\n",
    "\n",
    "def ocr_pdf(in_path):\n",
    "    \"\"\"Run OCR on PDF using OCRmyPDF with Arabic and English\"\"\"\n",
    "    tmp = tempfile.mkdtemp()\n",
    "    out_path = os.path.join(tmp, \"ocr.pdf\")\n",
    "    \n",
    "    try:\n",
    "        cmd = [\n",
    "            \"ocrmypdf\", \n",
    "            \"-l\", \"ara+eng\",  # Arabic + English\n",
    "            \"--rotate-pages\",  # Auto-rotate pages\n",
    "            \"--deskew\",        # Fix skewed scans\n",
    "            \"--clean\",         # Clean background\n",
    "            \"--force-ocr\",     # Force OCR even if text exists\n",
    "            in_path, \n",
    "            out_path\n",
    "        ]\n",
    "        subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "        return out_path, tmp\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"    ‚ö†Ô∏è  OCR failed: {e.stderr}\")\n",
    "        shutil.rmtree(tmp, ignore_errors=True)\n",
    "        return None, None\n",
    "    except FileNotFoundError:\n",
    "        print(\"    ‚ö†Ô∏è  OCRmyPDF not found. Install with: pip install ocrmypdf\")\n",
    "        print(\"    ‚ö†Ô∏è  Also requires Tesseract: https://github.com/tesseract-ocr/tesseract\")\n",
    "        shutil.rmtree(tmp, ignore_errors=True)\n",
    "        return None, None\n",
    "\n",
    "# Process all PDFs in the folder\n",
    "print(\"üöÄ Starting Advanced PDF Text Extraction\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìÅ Source folder: {pdf_folder}\")\n",
    "print(f\"üìÅ Output folder: {output_folder}\")\n",
    "print(\"\\n‚öôÔ∏è Extraction Rules:\")\n",
    "for key, value in EXTRACTION_RULES.items():\n",
    "    print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "print(\"\\nüìã Processing Steps:\")\n",
    "print(\"   1. Check if PDF is text-based or image-based (scanned)\")\n",
    "print(\"   2. Skip image-based PDFs (no extractable text)\")\n",
    "print(\"   3. Extract text from text-based PDFs\")\n",
    "print(\"   4. Apply OCR fallback if text quality is poor\")\n",
    "print(\"   5. Save as TXT and DOCX with proper Arabic formatting\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith('.pdf')]\n",
    "\n",
    "if not pdf_files:\n",
    "    print(\"\\n‚ùå No PDF files found in the folder!\")\n",
    "else:\n",
    "    print(f\"\\nüìö Found {len(pdf_files)} PDF files\\n\")\n",
    "    \n",
    "    # Statistics tracking\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    for i, pdf_file in enumerate(pdf_files, 1):\n",
    "        print(f\"\\n[{i}/{len(pdf_files)}] {pdf_file}\")\n",
    "        \n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        base_name = os.path.splitext(pdf_file)[0]\n",
    "        \n",
    "        # Step 1: Check if PDF is text-based or image-based\n",
    "        print(\"  üîç Checking PDF type...\")\n",
    "        is_text_based, text_ratio, details = is_pdf_text_based(pdf_path, sample_pages=3)\n",
    "        print(f\"  {details}\")\n",
    "        \n",
    "        # Skip image-based PDFs (scanned documents)\n",
    "        if not is_text_based:\n",
    "            print(\"  ‚è≠Ô∏è  Skipping: Image-based PDF (scanned document - no extractable text)\")\n",
    "            print(\"  üí° Tip: Use OCR tools to convert scanned PDFs to text-based format first\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Step 2: Extract text from text-based PDF\n",
    "        extracted_data = extract_text_from_pdf(pdf_path, EXTRACTION_RULES)\n",
    "        \n",
    "        # Step 3: Check if OCR fallback is needed (for poor quality text)\n",
    "        if extracted_data and needs_ocr(extracted_data):\n",
    "            print(\"  üîÅ Text looks broken ‚Üí running OCR fallback (ara+eng)...\")\n",
    "            ocr_path, tmpdir = ocr_pdf(pdf_path)\n",
    "            \n",
    "            if ocr_path:\n",
    "                # Re-extract from OCR'd PDF\n",
    "                extracted_data = extract_text_from_pdf(ocr_path, EXTRACTION_RULES)\n",
    "                shutil.rmtree(tmpdir, ignore_errors=True)\n",
    "                print(\"  ‚úÖ OCR completed successfully\")\n",
    "        \n",
    "        # Step 4: Save extracted text\n",
    "        if extracted_data:\n",
    "            try:\n",
    "                # Save as TXT\n",
    "                txt_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "                save_to_txt(extracted_data, txt_path)\n",
    "                print(f\"  üíæ Saved TXT: {base_name}.txt\")\n",
    "                \n",
    "                # Save as DOCX\n",
    "                docx_path = os.path.join(output_folder, f\"{base_name}.docx\")\n",
    "                save_to_docx(extracted_data, docx_path)\n",
    "                print(f\"  üíæ Saved DOCX: {base_name}.docx\")\n",
    "                \n",
    "                processed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Failed to save: {e}\")\n",
    "                failed_count += 1\n",
    "        else:\n",
    "            print(f\"  ‚ùå No text extracted\")\n",
    "            failed_count += 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ Extraction Complete!\")\n",
    "print(f\"üìä Summary:\")\n",
    "print(f\"   ‚Ä¢ Total PDFs found: {len(pdf_files)}\")\n",
    "print(f\"   ‚Ä¢ Successfully processed: {processed_count}\")\n",
    "print(f\"   ‚Ä¢ Skipped (image-based): {skipped_count}\")\n",
    "print(f\"   ‚Ä¢ Failed: {failed_count}\")\n",
    "print(f\"üìÅ All extracted files saved to: {output_folder}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOpmEbBbjyXEIAcpJLB+X+7",
   "mount_file_id": "1t65HR0pHIZw1wFRxKxy-g40QsQfmVtp2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
